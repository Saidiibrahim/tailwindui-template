import { ArticleLayout } from '@/components/ArticleLayout'
import Image from 'next/image'
import projectSetup from '../../images/articles/article_01/image_01.png'
import dockerFile from '../../images/articles/article_01/image_02.png'
import dockerBuilding from '../../images/articles/article_01/image_03.png'

export const meta = {
  author: 'Ibrahim Saidi',
  date: '2022-11-04',
  title: 'How to Serve AI Models with FastAPI and Docker',
  description:
    'Learn to create a containerised FastAPI app serving a HuggingFace model',
}

export default (props) => <ArticleLayout meta={meta} {...props} />

FastAPI is a modern web framework for building APIs with Python 3.7 and above.
Itâ€™s high-performant and yet easy to learn and use. Companies like [Microsoft
and Uber love it so much that they use it in their machine-learning services](https://fastapi.tiangolo.com/).

In this tutorial, weâ€™ll learn how to serve machine learning models with FastAPI for
live inferencing. More specifically, weâ€™ll use a [GPT-2 transformers model from Hugging
Face](https://huggingface.co/gpt2?text=My+name+is+Mariama%2C+my+favorite) to generate text given a short sentence.

While the app weâ€™ll be creating is really simple, weâ€™ll also get to practice a very
important skill in MLOps â€” packaging ML models that can be shared and deployed anywhere[1].

## Creating the app
To get started, create a project in your IDE of choice. I prefer PyCharm for Python projects,
but feel free to use something like VSCode. Inside your IDE, set up your project like this:

<Image src={projectSetup} alt="" />

The `requirements.txt` file contains all the packages that our app depends on to be functional.
Letâ€™s list these packages inside your `requirements.txt` file.

```text
transformers==4.23.1
tensorflow==2.10.0
fastapi==0.85.1
uvicorn[standard]
```
We need TensorFlow and transformers to interact with the HuggingFace Models. And we, of course,
need FastAPI itself and the standard Uvicorn to serve the FastAPI web application.

Note: Your IDE will complain when you try to import packages you havenâ€™t installed. So quickly
install all the packages in the requirements.txt file before moving on.

```python3 -m pip install -r requirements.txt```

Letâ€™s put the code for our FastAPI web app will be in a main.py module inside a /mlapp directory.

```python
from transformers import pipeline
from fastapi import FastAPI, Response
from pydantic import BaseModel

generator = pipeline('text-generation', model='gpt2')

app = FastAPI()


class Body(BaseModel):
    text: str


@app.get('/')
def root():
    return Response("<h1>How to Serve AI Models with FastAPI and Docker</h1>")


@app.post('/generate')
def predict(body: Body):
    results = generator(body.text, max_length=35, num_return_sequences=1)
    return results[0]
```
If youâ€™re new to FastAPI, letâ€™s see what the code above ğŸ‘†ğŸ¾ is doing. We start by importing
the required packages for our application to run successfully.

Then we declare a generator using the pipeline() function from HuggingFaceâ€™s transformers.
This function allows us to use many pre-trained models from HuggingFace.

In line 7, we instantiate an instance of the FastAPI class called app.
With this app instance, we can decorate some functions to create endpoints for our application.

The first endpoint is simply the root of our application with some HTML to greet the user of our app.

The next endpoint allows us to post some text to the Uvicorn server. Then FastAPI will process the request
and return the prediction result given a body of text.

## Containerising our app with Docker: AKA Dockerising our app
Okay, hold on, hold onâ€¦.

![Alt Text](https://giphy.com/gifs/abcnetwork-hold-up-wait-a-minute-xULW8MYvpNOfMXfDH2)

ğŸ¤”ğŸ¤·ğŸ¾â€â™‚ ï¸At this point, you might be wondering why we even have to bundle our app into a container. Hereâ€™s why.

A container is a portable unit of software that combines the application and all its dependencies into a single package
thatâ€™s agnostic to the underlying host OS. It removes the need to build complex environments and simplifies the application
development-to-deployment process.[2]

Weâ€™ll use Docker to containerise our application into a portable Docker image that can be run in any environment, such as
AWS ECS or Azure ACI, consistently.

To give you a mental model of whatâ€™s happening when we containerise our app,

- we begin with a Dockerfile which contains all the instructions to create our Docker image ( which is what we can share or deploy to other services).
- Then we can run the application thatâ€™s packaged inside this docker image by simply running the image.
- When we say a container, we are referring to this running instance of the Docker imageğŸ‘ğŸ¾ğŸ‘ŒğŸ¾.

<Image src={dockerFile} alt="" />

Letâ€™s now create the Dockerfile for our application. As mentioned above, this file contains the instructions required to containerise our app.

```Dockerfile
FROM python:3.10

WORKDIR /code

COPY ./requirements.txt /code/requirements.txt

RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt

COPY ./mlapp /code/app

CMD ["uvicorn","app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

`FROM python:3.10` : In the Dockerfile, weâ€™re going to start with the Python 3.10 image.

`COPY ./requirements.txt /code/requirements.txt`: Then weâ€™ll copy the requirements into the container where weâ€™re building it.
We copy it into the directory called /code which will be at the very root.

`WORKDIR /code` : We change our working directory into `/code`.

`RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt`: Then we will install the requirements.

`COPY ./mlapp /code/app`: And finally, we copy the local contents of anything thatâ€™s in the `/mlapp` directory into the containerâ€™s `/code/app` directory.

`CMD [â€œuvicornâ€,â€app.main:appâ€, â€œ â€” hostâ€, â€œ0.0.0.0â€, â€œ â€” portâ€, â€œ8000â€]` : Finally, weâ€™ll run it with uvicorn on host 0.0.0.0 and
weâ€™re going to run the function called app thatâ€™s inside the main.py module.

## Interact with the API

FastAPI makes it easy to test our API by providing us with a `/docs` endpoint out of the box for us to interact with our API.

Weâ€™ve done all the hard work, and now whatâ€™s left is for us to build and run our docker image and interact with the FastAPI web app thatâ€™s bundled inside it.

**Make sure you have docker installed and is running on your machine before getting started**

First, we build the docker image to bundle our app thatâ€™s in `mlapp/main.py`.
You can give the image name by tagging it ( `-t mlops-fastapi` in the example below).

```bash
# Terminal / Run from project's root directory
docker build -t mlops-fastapi .
````
<Image src={dockerBuilding} alt="" />

Letâ€™s now use that image to run a Docker container on port 8000 of our local machine.

```bash
# Terminal
docker run -p 8000:8000 mlops-fastapi
````
[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/9iRAN1OpYd8/0.jpg)](http://www.youtube.com/watch?v=9iRAN1OpYd8)

## Closing remarks
And there you have it. Congrats on making it to the end ğŸ¥³.
Weâ€™ve successfully built a FastAPI application that can serve a Machine Learning model.
While what we did here was very simple, you have taken your first steps towards MLOps.

You can find the complete code [here](https://github.com/Saidiibrahim/FastAPI-Docker).

## Resources
[1]https://www.oreilly.com/videos/packaging-machine-learning/50116VIDEOPAIML/

[2] https://developer.nvidia.com/ai-hpc-containers

[3] https://www.youtube.com/watch?v=pTFZFxd4hOI&t=1010s